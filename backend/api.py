"""
FastAPI åç«¯ API
æä¾›å›¾ä¹¦æœç´¢æ¥å£
"""

# æ ‡å‡†åº“å¯¼å…¥
import sys
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional

# æ£€æŸ¥å¹¶å¯¼å…¥ç¬¬ä¸‰æ–¹åº“
try:
    from fastapi import FastAPI, HTTPException, status
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.responses import JSONResponse
    from pydantic import BaseModel, Field
    import uvicorn
except ImportError as e:
    # print("="*60)
    pass
    # print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…ï¼")
    # print("="*60)
    # print(f"é”™è¯¯ä¿¡æ¯: {e}")
    # print()
    # print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š")
    # print("pip install -i https://pypi.tuna.tsinghua.edu.cn/simple fastapi uvicorn[standard] pydantic")
    # print()
    # print("æˆ–è€…è¿è¡Œï¼š")
    # print("pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt")
    # print("="*60)
    sys.exit(1)

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ dangdang æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)


def find_available_port(start_port=8000, max_attempts=10):
    """æŸ¥æ‰¾å¯ç”¨ç«¯å£"""
    import socket
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('0.0.0.0', port))
                return port
        except OSError:
            continue
    return None

# å¯¼å…¥çˆ¬è™«æ¨¡å—
try:
    from dangdang import run_spider, DangDangSpider
    from fanqie import (
        run_recommend_spider, 
        run_detail_spider, 
        run_author_spider,
        FanQieRecommendSpider,
        FanQieDetailSpider,
        FanQieAuthorSpider
    )
    from db_config import MYSQL_CONFIG, USE_MYSQL
    from mysql_pool import MySQLPool
except ImportError as e:
    # print("="*60)
    pass
    # print("âŒ å¯¼å…¥ dangdang æ¨¡å—å¤±è´¥ï¼")
    # print("="*60)
    # print(f"é”™è¯¯ä¿¡æ¯: {e}")
    # print(f"å½“å‰è·¯å¾„: {os.getcwd()}")
    # print(f"çˆ¶ç›®å½•: {parent_dir}")
    # print()
    # print("è¯·ç¡®ä¿ dangdang.py æ–‡ä»¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•")
    # print("="*60)
    sys.exit(1)


app = FastAPI(
    title="å½“å½“ç½‘å›¾ä¹¦çˆ¬è™« API",
    description="æä¾›å›¾ä¹¦æœç´¢å’Œæ•°æ®çˆ¬å–åŠŸèƒ½",
    version="1.0.0"
)

# åˆå§‹åŒ– MySQL è¿æ¥æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if USE_MYSQL:
    try:
        MySQLPool.initialize(
            host=MYSQL_CONFIG.get('host', 'localhost'),
            port=MYSQL_CONFIG.get('port', 3306),
            user=MYSQL_CONFIG.get('user', 'root'),
            password=MYSQL_CONFIG.get('password', ''),
            database=MYSQL_CONFIG.get('database', 'dangdang_books'),
            mincached=2,
            maxcached=10,
            maxconnections=20
        )
    except Exception as e:
        # print(f"âš ï¸ MySQL è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
        pass
        # print("âš ï¸ å°†ç¦ç”¨æ•°æ®åº“å­˜å‚¨åŠŸèƒ½")
        USE_MYSQL = False

# é…ç½® CORS - å…è®¸å‰ç«¯è·¨åŸŸè®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥æŒ‡å®šå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚æ¨¡å‹"""
    keyword: str = Field(..., min_length=1, max_length=50, description="æœç´¢å…³é”®è¯")
    max_books: int = Field(default=20, ge=0, le=500, description="æœ€å¤§çˆ¬å–æ•°é‡ï¼ˆ0è¡¨ç¤ºçˆ¬å–æ‰€æœ‰ï¼‰")
    proxy: Optional[str] = Field(default=None, description="ä»£ç†åœ°å€ï¼ˆæ ¼å¼ï¼šhttp://ip:portï¼‰")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "keyword": "Python",
                "max_books": 20,
                "proxy": None
            }
        }
    }


class BookInfo(BaseModel):
    """å›¾ä¹¦ä¿¡æ¯æ¨¡å‹"""
    æ ‡é¢˜: str = ""
    ä½œè€…: str = ""
    å‡ºç‰ˆç¤¾: str = ""
    å‡ºç‰ˆæ—¶é—´: str = ""
    åŸä»·: str = ""
    ç°ä»·: str = ""
    ISBN: str = ""
    è¯„åˆ†: str = ""
    è¯„è®ºæ•°: str = ""
    ç®€ä»‹: str = ""
    å°é¢å›¾: str = ""
    è¯¦æƒ…é¡µURL: str = ""


class SearchResponse(BaseModel):
    """æœç´¢å“åº”æ¨¡å‹"""
    success: bool
    keyword: str
    count: int
    books: List[Dict]
    total_crawled: int = 0  # çˆ¬å–æ€»æ•°
    total_saved: int = 0  # ä¿å­˜æ€»æ•°
    total_duplicates: int = 0  # å»é‡æ€»æ•°
    dedup_key: str = ""  # å»é‡å…³é”®è¯
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "success": True,
                "keyword": "Python",
                "count": 2,
                "total_crawled": 20,
                "total_saved": 18,
                "total_duplicates": 2,
                "dedup_key": "æ ‡é¢˜ + ä½œè€…",
                "books": [
                    {
                        "æ ‡é¢˜": "Pythonç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ",
                        "ä½œè€…": "åŸƒé‡Œå…‹Â·é©¬ç‘Ÿæ–¯",
                        "å‡ºç‰ˆç¤¾": "äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾",
                        "ç°ä»·": "Â¥89.00"
                    }
                ]
            }
        }
    }


# çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œç”¨äºå¼‚æ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
executor = ThreadPoolExecutor(max_workers=3)


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "å½“å½“ç½‘å›¾ä¹¦çˆ¬è™« API",
        "version": "1.0.0",
        "endpoints": {
            "crawl": "/api/crawl",
            "books": "/api/books",
            "stats": "/api/stats",
            "docs": "/docs",
            "health": "/health"
        }
    }


@app.get("/health", include_in_schema=False)
async def health_check():
    """å¥åº·æ£€æŸ¥ï¼ˆä¸è®°å½•è®¿é—®æ—¥å¿—ï¼‰"""
    return {"status": "healthy"}


@app.post("/api/crawl/fanqie/recommend")
async def crawl_fanqie_recommend():
    """
    çˆ¬å–ç•ªèŒ„å°è¯´æ¨èåˆ—è¡¨ï¼ˆåªçˆ¬å–ä¹¦å+IDï¼‰
    
    è¿”å›:
        æ¨èä¹¦ç±åˆ—è¡¨
    """
    try:
        loop = asyncio.get_event_loop()
        
        try:
            results = await asyncio.wait_for(
                loop.run_in_executor(
                    executor,
                    lambda: run_recommend_spider(
                        use_mysql=USE_MYSQL,
                        max_books=50
                    )
                ),
                timeout=60.0
            )
        except asyncio.TimeoutError:
            results = {'books': [], 'total_crawled': 0}
        
        books = results.get('books', [])
        
        return {
            "success": True,
            "count": len(books),
            "books": books,
            "total_crawled": results.get('total_crawled', 0)
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        
        raise HTTPException(
            status_code=500,
            detail=f"çˆ¬å–å¤±è´¥: {str(e)}"
        )


@app.post("/api/crawl/fanqie/detail")
async def crawl_fanqie_detail(book_name: Optional[str] = None, book_id: Optional[str] = None):
    """
    çˆ¬å–ç•ªèŒ„å°è¯´è¯¦æƒ…ï¼ˆæ ¹æ®ä¹¦åæˆ–IDï¼‰
    
    å‚æ•°:
        book_name: ä¹¦å
        book_id: ä¹¦ç±ID
    
    è¿”å›:
        ä¹¦ç±è¯¦æƒ…
    """
    if not book_name and not book_id:
        raise HTTPException(status_code=400, detail="è¯·æä¾›ä¹¦åæˆ–ä¹¦ç±ID")
    
    try:
        loop = asyncio.get_event_loop()
        
        try:
            result = await asyncio.wait_for(
                loop.run_in_executor(
                    executor,
                    lambda: run_detail_spider(
                        book_name=book_name,
                        book_id=book_id,
                        use_mysql=USE_MYSQL
                    )
                ),
                timeout=60.0
            )
        except asyncio.TimeoutError:
            result = {'book': None, 'success': False}
        
        if result['success']:
            return {
                "success": True,
                "book": result['book']
            }
        else:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ä¹¦ç±")
    
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        
        raise HTTPException(
            status_code=500,
            detail=f"çˆ¬å–å¤±è´¥: {str(e)}"
        )


@app.post("/api/crawl/fanqie/author")
async def crawl_fanqie_author(author_name: str):
    """
    æœç´¢ä½œè€…çš„æ‰€æœ‰ä¹¦ç±
    
    å‚æ•°:
        author_name: ä½œè€…å
    
    è¿”å›:
        ä½œè€…çš„ä¹¦ç±åˆ—è¡¨
    """
    if not author_name:
        raise HTTPException(status_code=400, detail="è¯·æä¾›ä½œè€…å")
    
    try:
        loop = asyncio.get_event_loop()
        
        try:
            results = await asyncio.wait_for(
                loop.run_in_executor(
                    executor,
                    lambda: run_author_spider(
                        author_name=author_name,
                        use_mysql=USE_MYSQL,
                        max_books=50
                    )
                ),
                timeout=60.0
            )
        except asyncio.TimeoutError:
            results = {'books': [], 'total_crawled': 0, 'author': author_name}
        
        books = results.get('books', [])
        
        return {
            "success": True,
            "author": author_name,
            "count": len(books),
            "books": books,
            "total_crawled": results.get('total_crawled', 0)
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        
        raise HTTPException(
            status_code=500,
            detail=f"æœç´¢å¤±è´¥: {str(e)}"
        )


@app.get("/api/fanqie/recommend")
async def get_fanqie_recommend(limit: int = 100):
    """
    ä»æ•°æ®åº“è·å–æ¨èä¹¦ç±åˆ—è¡¨
    
    å‚æ•°:
        limit: è¿”å›æ•°é‡é™åˆ¶
    
    è¿”å›:
        æ¨èä¹¦ç±åˆ—è¡¨
    """
    try:
        books = MySQLPool.get_fanqie_recommend_list(limit=limit)
        
        return {
            "success": True,
            "count": len(books),
            "books": books
        }
    
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"
        )


@app.get("/api/fanqie/detail/{book_id}")
async def get_fanqie_detail(book_id: str):
    """
    ä»æ•°æ®åº“è·å–ä¹¦ç±è¯¦æƒ…
    
    å‚æ•°:
        book_id: ä¹¦ç±ID
    
    è¿”å›:
        ä¹¦ç±è¯¦æƒ…
    """
    try:
        book = MySQLPool.get_fanqie_book_detail(book_id)
        
        if book:
            return {
                "success": True,
                "book": book
            }
        else:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ä¹¦ç±")
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"
        )


@app.get("/api/fanqie/author/{author_name}")
async def get_fanqie_author_books(author_name: str):
    """
    ä»æ•°æ®åº“è·å–ä½œè€…çš„æ‰€æœ‰ä¹¦ç±
    
    å‚æ•°:
        author_name: ä½œè€…å
    
    è¿”å›:
        ä½œè€…çš„ä¹¦ç±åˆ—è¡¨
    """
    try:
        books = MySQLPool.get_fanqie_author_books(author_name)
        
        return {
            "success": True,
            "author": author_name,
            "count": len(books),
            "books": books
        }
    
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"
        )


@app.post("/api/crawl/fanqie", response_model=SearchResponse)
async def crawl_fanqie_books(request: SearchRequest):
    """
    çˆ¬å–ç•ªèŒ„å°è¯´ APIï¼ˆçˆ¬å–å¹¶ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    
    å‚æ•°:
        request: åŒ…å«æœç´¢å…³é”®è¯å’Œçˆ¬å–æ•°é‡çš„è¯·æ±‚ä½“
    
    è¿”å›:
        åŒ…å«å°è¯´åˆ—è¡¨çš„å“åº”
    """
    keyword = request.keyword.strip()
    max_books = request.max_books
    proxy = request.proxy.strip() if request.proxy else None
    
    if not keyword:
        raise HTTPException(status_code=400, detail="å…³é”®è¯ä¸èƒ½ä¸ºç©º")
    
    if len(keyword) > 50:
        raise HTTPException(status_code=400, detail="å…³é”®è¯è¿‡é•¿ï¼Œè¯·è¾“å…¥50å­—ä»¥å†…")
    
    if max_books < 0 or max_books > 500:
        raise HTTPException(status_code=400, detail="çˆ¬å–æ•°é‡å¿…é¡»åœ¨ 0-500 ä¹‹é—´ï¼ˆ0è¡¨ç¤ºçˆ¬å–æ‰€æœ‰ï¼‰")
    
    try:
        # åœ¨çº¿ç¨‹æ± ä¸­å¼‚æ­¥è¿è¡Œçˆ¬è™«ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
        loop = asyncio.get_event_loop()
        
        # ä½¿ç”¨ asyncio.wait_for æ·»åŠ è¶…æ—¶ä¿æŠ¤
        try:
            results = await asyncio.wait_for(
                loop.run_in_executor(
                    executor,
                    lambda: run_fanqie_spider(
                        keyword=keyword,
                        thread_count=3,
                        use_mysql=USE_MYSQL,
                        mysql_config=MYSQL_CONFIG,
                        max_books=max_books,
                        proxy=proxy
                    )
                ),
                timeout=90.0  # 90ç§’è¶…æ—¶
            )
        except asyncio.TimeoutError:
            # è¶…æ—¶åè¿”å›ç©ºç»“æœ
            results = []
        
        # ç¡®ä¿ results ä¸ä¸º None
        if results is None:
            results = {
                'books': [],
                'total_crawled': 0,
                'total_saved': 0,
                'total_duplicates': 0,
                'dedup_key': 'æ ‡é¢˜ + ä½œè€…'
            }
        
        books = results.get('books', [])
        
        response_data = SearchResponse(
            success=True,
            keyword=keyword,
            count=len(books),
            books=books,
            total_crawled=results.get('total_crawled', 0),
            total_saved=results.get('total_saved', 0),
            total_duplicates=results.get('total_duplicates', 0),
            dedup_key=results.get('dedup_key', 'æ ‡é¢˜ + ä½œè€…')
        )
        
        return response_data
    
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=504,
            detail="çˆ¬å–è¶…æ—¶ï¼Œè¯·å‡å°‘çˆ¬å–æ•°é‡æˆ–ç¨åé‡è¯•"
        )
    
    except Exception as e:
        # è®°å½•é”™è¯¯æ—¥å¿—
        import traceback
        traceback.print_exc()
        
        raise HTTPException(
            status_code=500,
            detail=f"çˆ¬å–å¤±è´¥: {str(e)}"
        )


@app.get("/api/books/fanqie", response_model=SearchResponse)
async def get_fanqie_books_from_db(keyword: Optional[str] = None, limit: int = 100):
    """
    ä»æ•°æ®åº“è·å–ç•ªèŒ„å°è¯´æ•°æ®
    
    å‚æ•°:
        keyword: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        limit: è¿”å›æ•°é‡é™åˆ¶
    
    è¿”å›:
        åŒ…å«å°è¯´åˆ—è¡¨çš„å“åº”
    """
    try:
        # æ ¹æ®å…³é”®è¯è·å–æ•°æ®
        if keyword:
            books = MySQLPool.get_fanqie_books_by_keyword(keyword.strip())
        else:
            books = MySQLPool.get_all_fanqie_books(limit=limit)
        
        return SearchResponse(
            success=True,
            keyword=keyword or "å…¨éƒ¨",
            count=len(books),
            books=books
        )
    
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"
        )


@app.post("/api/crawl", response_model=SearchResponse)
async def crawl_books(request: SearchRequest):
    """
    çˆ¬å–å›¾ä¹¦ APIï¼ˆçˆ¬å–å¹¶ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
    
    å‚æ•°:
        request: åŒ…å«æœç´¢å…³é”®è¯å’Œçˆ¬å–æ•°é‡çš„è¯·æ±‚ä½“
    
    è¿”å›:
        åŒ…å«å›¾ä¹¦åˆ—è¡¨çš„å“åº”
    """
    keyword = request.keyword.strip()
    max_books = request.max_books
    proxy = request.proxy.strip() if request.proxy else None
    
    if not keyword:
        raise HTTPException(status_code=400, detail="å…³é”®è¯ä¸èƒ½ä¸ºç©º")
    
    if len(keyword) > 50:
        raise HTTPException(status_code=400, detail="å…³é”®è¯è¿‡é•¿ï¼Œè¯·è¾“å…¥50å­—ä»¥å†…")
    
    if max_books < 0 or max_books > 500:
        raise HTTPException(status_code=400, detail="çˆ¬å–æ•°é‡å¿…é¡»åœ¨ 0-500 ä¹‹é—´ï¼ˆ0è¡¨ç¤ºçˆ¬å–æ‰€æœ‰ï¼‰")
    
    if max_books == 0:
        # print(f"\n{'='*60}")
        pass
        # print(f"ğŸ“¥ æ”¶åˆ°çˆ¬å–è¯·æ±‚: å…³é”®è¯='{keyword}', æ¨¡å¼=æ— é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰ï¼‰")
        if proxy:
            # print(f"ğŸ”’ ä»£ç†è®¾ç½®: {proxy}")
            pass
        # print(f"{'='*60}\n")
    else:
        # print(f"\n{'='*60}")
        pass
        # print(f"ğŸ“¥ æ”¶åˆ°çˆ¬å–è¯·æ±‚: å…³é”®è¯='{keyword}', æ•°é‡={max_books}")
        if proxy:
            # print(f"ğŸ”’ ä»£ç†è®¾ç½®: {proxy}")
            pass
        # print(f"{'='*60}\n")
    
    try:
        # åœ¨çº¿ç¨‹æ± ä¸­å¼‚æ­¥è¿è¡Œçˆ¬è™«ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
        loop = asyncio.get_event_loop()
        
        # print("ğŸ”„ å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡...")
        
        # ä½¿ç”¨ asyncio.wait_for æ·»åŠ è¶…æ—¶ä¿æŠ¤
        try:
            results = await asyncio.wait_for(
                loop.run_in_executor(
                    executor,
                    lambda: run_spider(
                        keyword=keyword,
                        thread_count=3,
                        use_mysql=USE_MYSQL,
                        mysql_config=MYSQL_CONFIG,
                        max_books=max_books,
                        proxy=proxy
                    )
                ),
                timeout=90.0  # 90ç§’è¶…æ—¶ï¼ˆä»180ç§’å‡å°‘ï¼‰
            )
        except asyncio.TimeoutError:
            # print("âš ï¸ çˆ¬è™«ä»»åŠ¡è¶…æ—¶ï¼Œå¼ºåˆ¶è¿”å›")
            pass
            # è¶…æ—¶åè¿”å›ç©ºç»“æœ
            results = []
        
        # print(f"ğŸ”„ çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œè¿”å› {len(results) if results else 0} æ¡ç»“æœ")
        
        # ç¡®ä¿ results ä¸ä¸º None
        if results is None:
            results = {
                'books': [],
                'total_crawled': 0,
                'total_saved': 0,
                'total_duplicates': 0,
                'dedup_key': 'æ ‡é¢˜ + ä½œè€…'
            }
        
        books = results.get('books', [])
        
        # print(f"\n{'='*60}")
        # print(f"âœ… çˆ¬å–è¯·æ±‚å®Œæˆ:")
        # print(f"   çˆ¬å–æ•°é‡: {results.get('total_crawled', 0)} æœ¬")
        # print(f"   ä¿å­˜æ•°é‡: {results.get('total_saved', 0)} æœ¬")
        # print(f"   å»é‡æ•°é‡: {results.get('total_duplicates', 0)} æœ¬")
        # print(f"   å»é‡å…³é”®è¯: {results.get('dedup_key', 'æ ‡é¢˜ + ä½œè€…')}")
        # print(f"{'='*60}\n")
        
        response_data = SearchResponse(
            success=True,
            keyword=keyword,
            count=len(books),
            books=books,
            total_crawled=results.get('total_crawled', 0),
            total_saved=results.get('total_saved', 0),
            total_duplicates=results.get('total_duplicates', 0),
            dedup_key=results.get('dedup_key', 'æ ‡é¢˜ + ä½œè€…')
        )
        
        # print(f"ğŸ“¤ å‡†å¤‡è¿”å›å“åº”: success={response_data.success}, count={response_data.count}, saved={response_data.total_saved}")
        return response_data
    
    except asyncio.TimeoutError:
        # print(f"\n{'='*60}")
        pass
        # print(f"âš ï¸ è¯·æ±‚è¶…æ—¶")
        # print(f"{'='*60}\n")
        raise HTTPException(
            status_code=504,
            detail="çˆ¬å–è¶…æ—¶ï¼Œè¯·å‡å°‘çˆ¬å–æ•°é‡æˆ–ç¨åé‡è¯•"
        )
    
    except Exception as e:
        # è®°å½•é”™è¯¯æ—¥å¿—
        # print(f"\n{'='*60}")
        # print(f"âŒ çˆ¬å–é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        # print(f"{'='*60}\n")
        
        raise HTTPException(
            status_code=500,
            detail=f"çˆ¬å–å¤±è´¥: {str(e)}"
        )


@app.get("/api/books", response_model=SearchResponse)
async def get_books_from_db(keyword: Optional[str] = None, limit: int = 100):
    """
    ä»æ•°æ®åº“è·å–å›¾ä¹¦æ•°æ®
    
    å‚æ•°:
        keyword: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        limit: è¿”å›æ•°é‡é™åˆ¶
    
    è¿”å›:
        åŒ…å«å›¾ä¹¦åˆ—è¡¨çš„å“åº”
    """
    try:
        # æ ¹æ®å…³é”®è¯è·å–æ•°æ®
        if keyword:
            books = MySQLPool.get_books_by_keyword(keyword.strip())
        else:
            books = MySQLPool.get_all_books(limit=limit)
        
        return SearchResponse(
            success=True,
            keyword=keyword or "å…¨éƒ¨",
            count=len(books),
            books=books
        )
    
    except Exception as e:
        # print(f"æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"
        )


@app.get("/api/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = MySQLPool.get_statistics()
        
        return {
            "success": True,
            "total_books": stats.get('total_books', 0),
            "keywords": stats.get('keywords', []),
            "status": "running"
        }
    except Exception as e:
        return {
            "success": False,
            "total_books": 0,
            "keywords": [],
            "status": "running",
            "error": str(e)
        }


@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """å…¨å±€å¼‚å¸¸å¤„ç†"""
    # print(f"å…¨å±€å¼‚å¸¸: {str(exc)}")
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "success": False,
            "detail": f"æœåŠ¡å™¨é”™è¯¯: {str(exc)}"
        }
    )


def cleanup():
    """æ¸…ç†èµ„æº"""
    # print("\nğŸ§¹ æ­£åœ¨æ¸…ç†èµ„æº...")
    
    # å…³é—­çº¿ç¨‹æ± 
    try:
        executor.shutdown(wait=False, cancel_futures=True)
        # print("âœ… çº¿ç¨‹æ± å·²å…³é—­")
    except Exception as e:
        # print(f"âš ï¸ å…³é—­çº¿ç¨‹æ± å¤±è´¥: {e}")
        pass
    
    # å…³é—­æ•°æ®åº“è¿æ¥æ± 
    if USE_MYSQL:
        try:
            if MySQLPool._pool:
                MySQLPool._pool.close()
                # print("âœ… æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")
        except Exception as e:
            # print(f"âš ï¸ å…³é—­æ•°æ®åº“è¿æ¥æ± å¤±è´¥: {e}")
            pass


if __name__ == "__main__":
    import signal
    import atexit
    import logging
    
    # é…ç½®æ—¥å¿—è¿‡æ»¤å™¨ï¼Œè¿‡æ»¤æ‰ /health è¯·æ±‚çš„æ—¥å¿—
    class HealthCheckFilter(logging.Filter):
        def filter(self, record):
            # è¿‡æ»¤æ‰åŒ…å« /health çš„æ—¥å¿—
            return '/health' not in record.getMessage()
    
    # è·å– uvicorn çš„è®¿é—®æ—¥å¿—è®°å½•å™¨å¹¶æ·»åŠ è¿‡æ»¤å™¨
    logging.getLogger("uvicorn.access").addFilter(HealthCheckFilter())
    
    # æ³¨å†Œé€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°
    atexit.register(cleanup)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    def signal_handler(sig, frame):
        # print("\n")
        # print("="*60)
        # print("ğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æœåŠ¡å™¨...")
        # print("="*60)
        cleanup()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # æŸ¥æ‰¾å¯ç”¨ç«¯å£
    port = find_available_port(8001, 10)
    
    if port is None:
        # print("="*60)
        pass
        # print("âŒ é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ï¼ˆ8000-8009 éƒ½è¢«å ç”¨ï¼‰")
        # print("="*60)
        # print("è¯·å…³é—­å ç”¨ç«¯å£çš„ç¨‹åº")
        # print("="*60)
        sys.exit(1)
    
    # print("="*60)
    # print("ğŸš€ å½“å½“ç½‘å›¾ä¹¦çˆ¬è™« API å¯åŠ¨ä¸­...")
    # print("="*60)
    # print(f"ğŸ“ API åœ°å€: http://127.0.0.1:{port}")
    # print(f"ğŸ“ API æ–‡æ¡£: http://127.0.0.1:{port}/docs")
    # print(f"ğŸ“ å¥åº·æ£€æŸ¥: http://127.0.0.1:{port}/health")
    # print(f"ğŸ“ çˆ¬å–æ¥å£: http://127.0.0.1:{port}/api/crawl")
    # print(f"ğŸ“ å±•ç¤ºæ¥å£: http://127.0.0.1:{port}/api/books")
    # print(f"ğŸ“ ç»Ÿè®¡æ¥å£: http://127.0.0.1:{port}/api/stats")
    
    if port != 8000:
        # print(f"âš ï¸  æ³¨æ„ï¼šç«¯å£ 8000 è¢«å ç”¨ï¼Œä½¿ç”¨ç«¯å£ {port}")
        pass
        # print(f"âš ï¸  è¯·ä¿®æ”¹å‰ç«¯ app.js ä¸­çš„ API_BASE_URL ä¸º: http://127.0.0.1:{port}")
    
    # print("="*60)
    # print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    # print("="*60)
    # print()
    
    try:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=port,
            log_level="info",
            access_log=True
        )
    except KeyboardInterrupt:
        # print("\n")
        pass
        # print("="*60)
        # print("âœ… æœåŠ¡å™¨å·²åœæ­¢")
        # print("="*60)
    except Exception as e:
        # print("\n")
        pass
        # print("="*60)
        # print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        # print("="*60)
        sys.exit(1)
    finally:
        cleanup()
