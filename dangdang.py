"""
å½“å½“ç½‘å›¾ä¹¦çˆ¬è™« - ä½¿ç”¨feapderæ¡†æ¶
åŠŸèƒ½ï¼šæ ¹æ®å…³é”®è¯æœç´¢å›¾ä¹¦ï¼Œå¹¶çˆ¬å–è¯¦æƒ…é¡µä¿¡æ¯
"""

import feapder
from feapder import Request
from typing import List, Dict, Optional
from mysql_pool import MySQLPool


class DangDangSpider(feapder.AirSpider):
    """å½“å½“ç½‘å›¾ä¹¦çˆ¬è™«"""
    
    # è‡ªå®šä¹‰é…ç½®
    __custom_setting__ = dict(
        SPIDER_THREAD_COUNT=3,  # çº¿ç¨‹æ•°ï¼ˆå‡å°‘çº¿ç¨‹æ•°ï¼Œæé«˜ç¨³å®šæ€§ï¼‰
        SPIDER_MAX_RETRY_TIMES=2,  # æœ€å¤§é‡è¯•æ¬¡æ•°
        REQUEST_TIMEOUT=30,  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        RETRY_FAILED_REQUESTS=False,  # ä¸é‡è¯•å¤±è´¥çš„è¯·æ±‚
        LOG_LEVEL="ERROR",  # åªæ˜¾ç¤ºé”™è¯¯æ—¥å¿—
    )
    
    def __init__(self, keyword="Python", use_mysql=True, max_books=20, proxy=None, *args, **kwargs):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param keyword: æœç´¢å…³é”®è¯
        :param use_mysql: æ˜¯å¦ä½¿ç”¨ MySQL å­˜å‚¨ï¼ˆé»˜è®¤ Trueï¼‰
        :param max_books: æœ€å¤§çˆ¬å–å›¾ä¹¦æ•°é‡ï¼ˆé»˜è®¤ 20ï¼Œ0è¡¨ç¤ºçˆ¬å–æ‰€æœ‰ï¼‰
        :param proxy: ä»£ç†åœ°å€ï¼ˆæ ¼å¼ï¼šhttp://ip:port æˆ– https://ip:portï¼‰
        """
        super().__init__(*args, **kwargs)
        self.keyword = keyword
        self.results = []  # å­˜å‚¨çˆ¬å–ç»“æœ
        self.use_mysql = use_mysql
        self.target_new_books = max_books  # ç›®æ ‡æ–°å¢æ•°é‡
        self.is_unlimited = (max_books == 0)  # æ˜¯å¦æ— é™åˆ¶æ¨¡å¼
        self.crawled_count = 0  # å·²çˆ¬å–æ•°é‡
        self._stop_flag = False  # åœæ­¢æ ‡å¿—
        self.saved_count = 0  # å®é™…ä¿å­˜åˆ°æ•°æ®åº“çš„æ•°é‡ï¼ˆæ–°å¢ï¼‰
        self.duplicate_count = 0  # å»é‡æ•°é‡
        self.max_crawl_limit = 1000  # æœ€å¤§çˆ¬å–é™åˆ¶ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        self.proxy = proxy  # ä»£ç†åœ°å€
        self.skipped_count = 0  # è·³è¿‡çš„è¯·æ±‚æ•°é‡ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
    
    def start_requests(self):
        """
        ç”Ÿæˆåˆå§‹è¯·æ±‚ - æœç´¢é¡µ
        """
        # æ„é€ æœç´¢URL
        search_url = f"https://search.dangdang.com/?key={self.keyword}&act=input"
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_kwargs = {
            "url": search_url,
            "headers": headers,
            "callback": self.parse_search_page
        }
        
        # å¦‚æœè®¾ç½®äº†ä»£ç†ï¼Œæ·»åŠ ä»£ç†é…ç½®
        if self.proxy:
            request_kwargs["proxies"] = {
                "http": self.proxy,
                "https": self.proxy
            }
            print(f"ğŸ”’ ä½¿ç”¨ä»£ç†: {self.proxy}")
        
        yield Request(**request_kwargs)
    
    def parse_search_page(self, request, response):
        """
        è§£ææœç´¢ç»“æœé¡µ
        æå–å›¾ä¹¦åˆ—è¡¨å’Œè¯¦æƒ…é¡µé“¾æ¥
        """
        print(f"ğŸ“„ æ­£åœ¨è§£ææœç´¢é¡µ: {response.url}")
        
        # æå–å›¾ä¹¦åˆ—è¡¨
        # æ–¹å¼1: å¤§å›¾æ¨¡å¼
        book_items = response.xpath('//ul[@class="bigimg"]/li')
        
        if not book_items:
            # æ–¹å¼2: åˆ—è¡¨æ¨¡å¼
            book_items = response.xpath('//ul[@id="component_59"]/li')
        
        if not book_items:
            # æ–¹å¼3: å…¶ä»–å¯èƒ½çš„åˆ—è¡¨
            book_items = response.xpath('//div[@id="search_nature_rg"]//li[@class="line1"]')
        
        print(f"ğŸ“š æ‰¾åˆ° {len(book_items)} ä¸ªå›¾ä¹¦é¡¹")
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
        if self._stop_flag:
            # å·²ç»åœæ­¢ï¼Œä¸å†å¤„ç†
            return
        
        if not self.is_unlimited and self.saved_count >= self.target_new_books:
            # å·²è¾¾åˆ°ç›®æ ‡ï¼Œä¸å†å¤„ç†
            if self.skipped_count == 0:
                print(f"\nâ­ï¸  å·²è¾¾åˆ°ç›®æ ‡ï¼Œè·³è¿‡æœç´¢é¡µå¤„ç†")
            return
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§çˆ¬å–é™åˆ¶
        if self.crawled_count >= self.max_crawl_limit:
            # å·²è¾¾åˆ°é™åˆ¶
            if self.skipped_count == 0:
                print(f"\nâ­ï¸  å·²è¾¾åˆ°æœ€å¤§çˆ¬å–é™åˆ¶ï¼Œè·³è¿‡æœç´¢é¡µå¤„ç†")
            return
        
        # å¤„ç†å›¾ä¹¦é¡¹
        count = 0
        for item in book_items:
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
            if self._stop_flag:
                # å·²ç»åœæ­¢ï¼Œä¸å†å¤„ç†
                return
            
            if not self.is_unlimited and self.saved_count >= self.target_new_books:
                # å·²è¾¾åˆ°ç›®æ ‡ï¼Œä¸å†å‘èµ·æ–°è¯·æ±‚
                return
            
            # æå–è¯¦æƒ…é¡µé“¾æ¥
            detail_url = item.xpath('.//a[@class="pic"]/@href').extract_first() or \
                        item.xpath('.//p[@class="name"]/a/@href').extract_first() or \
                        item.xpath('.//a[@name="itemlist-title"]/@href').extract_first()
            
            # æå–åŸºæœ¬ä¿¡æ¯ï¼ˆæœç´¢é¡µå¯è§çš„ä¿¡æ¯ï¼‰
            title = item.xpath('.//a[@class="pic"]/@title').extract_first() or \
                   item.xpath('.//p[@class="name"]/a/@title').extract_first() or \
                   item.xpath('.//a[@name="itemlist-title"]/@title').extract_first()
            
            price = item.xpath('.//p[@class="price"]/span[@class="search_now_price"]/text()').extract_first() or \
                   item.xpath('.//span[@class="search_now_price"]/text()').extract_first()
            
            if detail_url:
                count += 1
                
                # å†æ¬¡æ£€æŸ¥ï¼ˆåœ¨å‘èµ·è¯·æ±‚å‰ï¼‰
                if self._stop_flag or (not self.is_unlimited and self.saved_count >= self.target_new_books):
                    # ä¸å†å‘èµ·æ–°è¯·æ±‚
                    return
                # æ„å»ºè¯·æ±‚å‚æ•°
                request_kwargs = {
                    "url": detail_url,
                    "headers": request.headers,
                    "callback": self.parse_detail_page,
                    "meta": {"title": title, "price": price}
                }
                
                # å¦‚æœè®¾ç½®äº†ä»£ç†ï¼Œæ·»åŠ ä»£ç†é…ç½®
                if self.proxy:
                    request_kwargs["proxies"] = {
                        "http": self.proxy,
                        "https": self.proxy
                    }
                
                # å‘èµ·è¯¦æƒ…é¡µè¯·æ±‚
                yield Request(**request_kwargs)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦ç¿»é¡µ
        should_continue = False
        
        # æ£€æŸ¥åœæ­¢æ ‡å¿—
        if self._stop_flag:
            # å·²åœæ­¢ï¼Œä¸å†ç¿»é¡µ
            return
        
        if self.is_unlimited:
            # æ— é™åˆ¶æ¨¡å¼ï¼šç»§ç»­ç¿»é¡µç›´åˆ°æ²¡æœ‰æ›´å¤šæ•°æ®
            should_continue = True
        else:
            # é™åˆ¶æ¨¡å¼ï¼šå¦‚æœæ–°å¢æ•°é‡æœªè¾¾åˆ°ç›®æ ‡ï¼Œç»§ç»­ç¿»é¡µ
            if self.saved_count < self.target_new_books and self.crawled_count < self.max_crawl_limit:
                should_continue = True
            else:
                if self.saved_count >= self.target_new_books:
                    # å·²è¾¾åˆ°ç›®æ ‡ï¼Œä¸å†ç¿»é¡µ
                    return
        
        # å°è¯•ç¿»é¡µ
        if should_continue:
            next_page = response.xpath('//li[@class="next"]/a/@href').extract_first()
            if next_page:
                if self.is_unlimited:
                    print(f"ğŸ“„ æ— é™åˆ¶æ¨¡å¼ï¼Œç»§ç»­ç¿»é¡µ: {next_page}")
                else:
                    print(f"ğŸ“„ æ–°å¢æ•°é‡ {self.saved_count}/{self.target_new_books}ï¼Œç»§ç»­ç¿»é¡µ: {next_page}")
                
                # æ„å»ºè¯·æ±‚å‚æ•°
                request_kwargs = {
                    "url": response.urljoin(next_page),
                    "headers": request.headers,
                    "callback": self.parse_search_page
                }
                
                # å¦‚æœè®¾ç½®äº†ä»£ç†ï¼Œæ·»åŠ ä»£ç†é…ç½®
                if self.proxy:
                    request_kwargs["proxies"] = {
                        "http": self.proxy,
                        "https": self.proxy
                    }
                
                yield Request(**request_kwargs)
            else:
                if self.is_unlimited:
                    print(f"ğŸ“„ å·²åˆ°æœ€åä¸€é¡µï¼Œæ— æ›´å¤šæ•°æ®")
                else:
                    print(f"ğŸ“„ å·²åˆ°æœ€åä¸€é¡µï¼Œå®é™…æ–°å¢ {self.saved_count} æœ¬ï¼ˆç›®æ ‡ {self.target_new_books} æœ¬ï¼‰")

    def parse_detail_page(self, request, response):
        """
        è§£æå›¾ä¹¦è¯¦æƒ…é¡µ
        æå–å®Œæ•´çš„å›¾ä¹¦ä¿¡æ¯
        """
        try:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢ï¼ˆéæ— é™åˆ¶æ¨¡å¼ä¸”å·²è¾¾åˆ°ç›®æ ‡ï¼‰
            if not self.is_unlimited and self.saved_count >= self.target_new_books:
                # è®°å½•è·³è¿‡æ•°é‡
                self.skipped_count += 1
                # åªåœ¨ç¬¬ä¸€æ¬¡è·³è¿‡æ—¶æ‰“å°æç¤º
                if self.skipped_count == 1:
                    print(f"\nâ­ï¸  å·²è¾¾åˆ°ç›®æ ‡æ–°å¢æ•°é‡ {self.target_new_books}ï¼Œåç»­è¯·æ±‚å°†è¢«è·³è¿‡...")
                return
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§çˆ¬å–é™åˆ¶
            if self.crawled_count >= self.max_crawl_limit:
                self.skipped_count += 1
                if self.skipped_count == 1:
                    print(f"\nâ­ï¸  å·²è¾¾åˆ°æœ€å¤§çˆ¬å–é™åˆ¶ {self.max_crawl_limit}ï¼Œåç»­è¯·æ±‚å°†è¢«è·³è¿‡...")
                return
            
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self._stop_flag:
                self.skipped_count += 1
                return
            
            # æ‰“å°æ­£åœ¨è§£æçš„URL
            print(f"ğŸ” æ­£åœ¨è§£æè¯¦æƒ…é¡µ: {response.url}")
            
            # ä»metaä¸­è·å–æœç´¢é¡µçš„åŸºæœ¬ä¿¡æ¯
            basic_title = request.meta.get("title", "")
            basic_price = request.meta.get("price", "")
            
            # æå–è¯¦æƒ…é¡µä¿¡æ¯
            # å›¾ä¹¦æ ‡é¢˜ - å¤šç§æ–¹å¼å°è¯•
            title = response.xpath('//div[@class="name_info"]//h1/@title').extract_first() or \
                   response.xpath('//div[@class="name_info"]//h1/text()').extract_first() or \
                   response.xpath('//h1[@class="title"]/text()').extract_first() or \
                   basic_title
            
            # ä½œè€… - å¤šç§æ–¹å¼å°è¯•
            author = response.xpath('//span[@id="author"]//a/text()').extract_first() or \
                    response.xpath('//div[@class="messbox_info"]//span[contains(text(),"ä½œ")]/following-sibling::a[1]/text()').extract_first() or \
                    response.xpath('//a[@name="itemlist-author"]/text()').extract_first() or \
                    response.xpath('//p[@class="author"]//a[1]/text()').extract_first()
            
            # å‡ºç‰ˆç¤¾ - å¤šç§æ–¹å¼å°è¯•
            publisher = response.xpath('//span[@id="publisher"]//a/text()').extract_first() or \
                       response.xpath('//div[@class="messbox_info"]//span[contains(text(),"å‡ºç‰ˆç¤¾")]/following-sibling::a[1]/text()').extract_first() or \
                       response.xpath('//a[@name="P_cbs"]/text()').extract_first()
            
            # å‡ºç‰ˆæ—¶é—´ - å¤šç§æ–¹å¼å°è¯•
            publish_date = response.xpath('//span[@id="publish_time"]/text()').extract_first() or \
                          response.xpath('//div[@class="messbox_info"]//span[contains(text(),"å‡ºç‰ˆæ—¶é—´")]/following-sibling::text()[1]').extract_first() or \
                          response.xpath('//span[@name="P_date"]/text()').extract_first()
            
            # ä»·æ ¼ä¿¡æ¯
            original_price = response.xpath('//span[@id="original-price"]/text()').extract_first() or \
                            response.xpath('//p[@class="price"]/span[@class="price_n"]/text()').extract_first()
            
            current_price = response.xpath('//span[@id="dd-price"]/text()').extract_first() or \
                           basic_price
            
            # å›¾ä¹¦ç®€ä»‹
            description = response.xpath('//div[@class="descrip"]//text()').extract_first() or \
                         response.xpath('//div[@id="content"]//div[@class="describe_detail"]//text()').extract_first() or \
                         response.xpath('//div[@class="book_intro"]//text()').extract_first()
            
            # ISBN - å¤šç§æ–¹å¼å°è¯•
            isbn = response.xpath('//li[contains(text(),"ISBN")]/text()').extract_first()
            if not isbn:
                isbn = response.xpath('//span[contains(text(),"ISBN")]/following-sibling::text()[1]').extract_first()
            if isbn:
                isbn = isbn.replace("ISBNï¼š", "").replace("ISBN:", "").strip()
            
            # è¯„åˆ† - å¤šç§æ–¹å¼å°è¯•
            rating = response.xpath('//span[@class="star_gray"]/text()').extract_first() or \
                    response.xpath('//div[@class="star"]//text()').extract_first() or \
                    response.xpath('//span[@class="score"]/text()').extract_first()
            
            # è¯„è®ºæ•°
            comment_count = response.xpath('//span[@id="comm_num_down"]/text()').extract_first() or \
                           response.xpath('//a[@id="comm_num"]/text()').extract_first()
            
            # å›¾ä¹¦å°é¢
            cover_image = response.xpath('//img[@id="largePic"]/@src').extract_first() or \
                         response.xpath('//div[@class="pic_box"]//img/@src').extract_first() or \
                         response.xpath('//img[@id="main-img"]/@src').extract_first()
            
            # æ¸…ç†æ•°æ®
            if title:
                title = title.strip()
            if author:
                author = author.strip()
            if publisher:
                publisher = publisher.strip()
            if publish_date:
                publish_date = publish_date.strip()
            if description:
                description = description.strip()
            
            # æ„é€ å›¾ä¹¦æ•°æ®
            book_data = {
                "æ ‡é¢˜": title if title else "",
                "ä½œè€…": author if author else "",
                "å‡ºç‰ˆç¤¾": publisher if publisher else "",
                "å‡ºç‰ˆæ—¶é—´": publish_date if publish_date else "",
                "åŸä»·": original_price.strip() if original_price else "",
                "ç°ä»·": current_price.strip() if current_price else "",
                "ISBN": isbn.strip() if isbn else "",
                "è¯„åˆ†": rating.strip() if rating else "",
                "è¯„è®ºæ•°": comment_count.strip() if comment_count else "",
                "ç®€ä»‹": description if description else "",
                "å°é¢å›¾": cover_image.strip() if cover_image else "",
                "è¯¦æƒ…é¡µURL": response.url,
                "æœç´¢å…³é”®è¯": self.keyword  # æ·»åŠ æœç´¢å…³é”®è¯
            }
            
            # æ‰“å°æå–çš„ä¿¡æ¯ç”¨äºè°ƒè¯•
            print(f"ğŸ“– æå–ä¿¡æ¯: æ ‡é¢˜={title}, ä½œè€…={author}, å‡ºç‰ˆç¤¾={publisher}, å‡ºç‰ˆæ—¶é—´={publish_date}, è¯„åˆ†={rating}")
            
            # å­˜å‚¨åˆ°å†…å­˜
            self.results.append(book_data)
            self.crawled_count += 1
            
            # å­˜å‚¨åˆ° MySQLï¼ˆä½¿ç”¨è¿æ¥æ± ï¼‰
            is_new = False
            if self.use_mysql:
                try:
                    result = MySQLPool.save_book(book_data)
                    if result['success']:
                        self.saved_count += 1
                        is_new = True
                        if self.is_unlimited:
                            print(f"ğŸ’¾ æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå·²æ–°å¢: {self.saved_count}ï¼Œå·²çˆ¬å–: {self.crawled_count}ï¼‰")
                        else:
                            print(f"ğŸ’¾ æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå·²æ–°å¢: {self.saved_count}/{self.target_new_books}ï¼Œå·²çˆ¬å–: {self.crawled_count}ï¼‰")
                    elif result['is_duplicate']:
                        self.duplicate_count += 1
                        print(f"âš ï¸ å›¾ä¹¦é‡å¤ï¼Œå·²è·³è¿‡ï¼ˆå»é‡: {self.duplicate_count}ï¼Œå·²çˆ¬å–: {self.crawled_count}ï¼‰")
                    else:
                        print(f"âš ï¸ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {result['message']}")
                except Exception as e:
                    print(f"âš ï¸ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            else:
                # ä¸ä½¿ç”¨æ•°æ®åº“æ—¶ï¼Œæ‰€æœ‰æ•°æ®éƒ½ç®—æ–°å¢
                self.saved_count += 1
                is_new = True
            
            # æ˜¾ç¤ºè¿›åº¦
            if self.is_unlimited:
                print(f"âœ… å·²çˆ¬å– {self.crawled_count} æœ¬å›¾ä¹¦ï¼ˆæ–°å¢: {self.saved_count}ï¼Œé‡å¤: {self.duplicate_count}ï¼‰")
            else:
                print(f"âœ… å·²çˆ¬å– {self.crawled_count} æœ¬å›¾ä¹¦ï¼ˆæ–°å¢: {self.saved_count}/{self.target_new_books}ï¼Œé‡å¤: {self.duplicate_count}ï¼‰")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡ï¼ˆéæ— é™åˆ¶æ¨¡å¼ï¼‰
            if not self.is_unlimited and self.saved_count >= self.target_new_books:
                # åªåœ¨åˆšè¾¾åˆ°ç›®æ ‡æ—¶æ‰“å°ä¸€æ¬¡
                if not self._stop_flag:
                    print(f"\n{'='*60}")
                    print(f"ğŸ‰ å·²å®Œæˆç›®æ ‡ï¼æˆåŠŸæ–°å¢ {self.saved_count} æœ¬å›¾ä¹¦")
                    print(f"ğŸ“Š æ€»çˆ¬å–: {self.crawled_count} æœ¬ï¼Œå»é‡: {self.duplicate_count} æœ¬")
                    print(f"ğŸ›‘ æ­£åœ¨åœæ­¢çˆ¬è™«...")
                    print(f"{'='*60}\n")
                    # ä¸»åŠ¨åœæ­¢çˆ¬è™«
                    self._stop_crawling()
                return
        
        except Exception as e:
            print(f"âŒ è§£æè¯¦æƒ…é¡µå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # ç»§ç»­å¤„ç†å…¶ä»–é¡µé¢ï¼Œä¸ä¸­æ–­çˆ¬è™«
    
    def _stop_crawling(self):
        """åœæ­¢çˆ¬è™«çš„å†…éƒ¨æ–¹æ³•"""
        if self._stop_flag:
            # å·²ç»åœæ­¢è¿‡äº†ï¼Œä¸é‡å¤æ‰“å°
            return
        
        try:
            self._stop_flag = True
            print("ğŸ›‘ çˆ¬è™«å·²åœæ­¢")
            
            # æ‰“å°è·³è¿‡ç»Ÿè®¡
            if self.skipped_count > 0:
                print(f"ğŸ“Š è·³è¿‡äº† {self.skipped_count} ä¸ªå·²åœ¨é˜Ÿåˆ—ä¸­çš„è¯·æ±‚")
            
            # è°ƒç”¨çˆ¶ç±»çš„åœæ­¢æ–¹æ³•
            if hasattr(self, '_spider') and self._spider:
                self._spider.stop()
        except Exception as e:
            print(f"âš ï¸ åœæ­¢çˆ¬è™«æ—¶å‡ºé”™: {e}")
    
    def stop(self):
        """åœæ­¢çˆ¬è™«ï¼ˆå…¬å…±æ–¹æ³•ï¼‰"""
        self._stop_crawling()


def run_spider(keyword: str, thread_count: int = 3, use_mysql: bool = True, mysql_config: Optional[Dict] = None, max_books: int = 20, proxy: Optional[str] = None) -> Dict:
    """
    è¿è¡Œçˆ¬è™«å¹¶è¿”å›ç»“æœ
    :param keyword: æœç´¢å…³é”®è¯
    :param thread_count: çº¿ç¨‹æ•°
    :param use_mysql: æ˜¯å¦ä½¿ç”¨ MySQL å­˜å‚¨ï¼ˆé»˜è®¤ Trueï¼‰
    :param mysql_config: MySQL é…ç½®å­—å…¸ï¼ˆç”¨äºåˆå§‹åŒ–è¿æ¥æ± ï¼‰
    :param max_books: æœ€å¤§çˆ¬å–å›¾ä¹¦æ•°é‡ï¼ˆé»˜è®¤ 20ï¼‰
    :return: å›¾ä¹¦æ•°æ®åˆ—è¡¨
    """
    import time
    import threading
    
    print("\n" + "="*60)
    print(f"ğŸš€ å¼€å§‹çˆ¬å–å…³é”®è¯: {keyword}")
    print(f"ğŸ“Š çº¿ç¨‹æ•°: {thread_count}")
    if max_books == 0:
        print(f"ğŸ“š çˆ¬å–æ¨¡å¼: æ— é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰æ•°æ®ï¼‰")
    else:
        print(f"ğŸ“š ç›®æ ‡æ–°å¢æ•°é‡: {max_books} æœ¬")
        print(f"ğŸ“Œ è¯´æ˜: ä¼šæŒç»­çˆ¬å–ç›´åˆ°æ–°å¢ {max_books} æœ¬åˆ°æ•°æ®åº“ï¼ˆè‡ªåŠ¨å»é‡ï¼‰")
    print(f"ğŸ’¾ æ•°æ®åº“å­˜å‚¨: {'å¯ç”¨' if use_mysql else 'ç¦ç”¨'}")
    if proxy:
        print(f"ğŸ”’ ä»£ç†è®¾ç½®: {proxy}")
    else:
        print(f"ğŸ”’ ä»£ç†è®¾ç½®: æœªä½¿ç”¨")
    print("="*60 + "\n")
    
    # å¦‚æœä½¿ç”¨ MySQLï¼Œå…ˆåˆå§‹åŒ–è¿æ¥æ± 
    if use_mysql:
        if mysql_config is None:
            # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            from db_config import MYSQL_CONFIG
            mysql_config = MYSQL_CONFIG
        
        try:
            MySQLPool.initialize(
                host=mysql_config.get('host', 'localhost'),
                port=mysql_config.get('port', 3306),
                user=mysql_config.get('user', 'root'),
                password=mysql_config.get('password', ''),
                database=mysql_config.get('database', 'dangdang_books'),
                mincached=2,
                maxcached=10,
                maxconnections=20
            )
        except Exception as e:
            print(f"âš ï¸ è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
            use_mysql = False
    
    spider = None
    spider_thread = None
    
    try:
        spider = DangDangSpider(
            keyword=keyword, 
            thread_count=thread_count,
            use_mysql=use_mysql,
            max_books=max_books,
            proxy=proxy
        )
        
        print(f"ğŸ•·ï¸ çˆ¬è™«å¼€å§‹è¿è¡Œ...")
        
        # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œçˆ¬è™«ï¼ˆé daemonï¼Œç¡®ä¿æ­£å¸¸å®Œæˆï¼‰
        def run_spider_thread():
            try:
                spider.start()
            except Exception as e:
                print(f"âš ï¸ çˆ¬è™«çº¿ç¨‹å¼‚å¸¸: {e}")
        
        spider_thread = threading.Thread(target=run_spider_thread, daemon=False)
        spider_thread.start()
        
        # ç­‰å¾…çˆ¬è™«å®Œæˆæˆ–è¾¾åˆ°ç›®æ ‡
        max_wait_time = 180  # æœ€å¤šç­‰å¾…180ç§’
        wait_interval = 0.5  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡
        elapsed = 0
        
        print(f"â³ ç­‰å¾…çˆ¬è™«å®Œæˆ...")
        
        while elapsed < max_wait_time:
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼ˆéæ— é™åˆ¶æ¨¡å¼ï¼‰
            if not spider.is_unlimited and spider.saved_count >= max_books:
                print(f"\n{'='*60}")
                print(f"âœ… å·²è¾¾åˆ°ç›®æ ‡æ–°å¢æ•°é‡ {max_books}ï¼Œå‡†å¤‡åœæ­¢çˆ¬è™«")
                print(f"ğŸ“Š å½“å‰çŠ¶æ€: çˆ¬å– {spider.crawled_count} æœ¬ï¼Œæ–°å¢ {spider.saved_count} æœ¬ï¼Œå»é‡ {spider.duplicate_count} æœ¬")
                print(f"{'='*60}\n")
                
                # è®¾ç½®åœæ­¢æ ‡å¿—
                spider._stop_flag = True
                
                # ä¸»åŠ¨åœæ­¢çˆ¬è™«
                try:
                    spider._stop_crawling()
                except Exception as e:
                    print(f"âš ï¸ åœæ­¢çˆ¬è™«æ—¶å‡ºé”™: {e}")
                
                # ç­‰å¾…çº¿ç¨‹ç»“æŸï¼ˆæœ€å¤š5ç§’ï¼‰
                print(f"â³ ç­‰å¾…çˆ¬è™«çº¿ç¨‹ç»“æŸ...")
                spider_thread.join(timeout=5)
                
                if spider_thread.is_alive():
                    print(f"âš ï¸ çˆ¬è™«çº¿ç¨‹æœªèƒ½åŠæ—¶ç»“æŸï¼Œå¼ºåˆ¶è¿”å›ç»“æœ")
                else:
                    print(f"âœ… çˆ¬è™«çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
                
                break
            
            # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦è¿˜æ´»ç€
            if not spider_thread.is_alive():
                print(f"âœ… çˆ¬è™«çº¿ç¨‹å·²è‡ªç„¶ç»“æŸ")
                break
            
            time.sleep(wait_interval)
            elapsed += wait_interval
        
        if elapsed >= max_wait_time:
            print(f"âš ï¸ ç­‰å¾…è¶…æ—¶ï¼ˆ{max_wait_time}ç§’ï¼‰ï¼Œå¼ºåˆ¶è¿”å›ç»“æœ")
            spider._stop_flag = True
            try:
                spider._stop_crawling()
            except:
                pass
        
        print(f"ğŸ•·ï¸ çˆ¬è™«è¿è¡Œç»“æŸ")
        
        result_count = len(spider.results) if spider and spider.results else 0
        saved_count = spider.saved_count if spider else 0
        duplicate_count = spider.duplicate_count if spider else 0
        
        print("\n" + "="*60)
        print(f"âœ… çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š çˆ¬å–æ•°é‡: {result_count} æœ¬")
        print(f"ğŸ’¾ ä¿å­˜æ•°é‡: {saved_count} æœ¬")
        print(f"ğŸ”„ å»é‡æ•°é‡: {duplicate_count} æœ¬")
        print(f"ğŸ“Œ å»é‡å…³é”®è¯: æ ‡é¢˜ + ä½œè€…")
        if use_mysql:
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° MySQL æ•°æ®åº“")
        print("="*60 + "\n")
        
        # ç¡®ä¿è¿”å›ç»“æœï¼ˆåŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼‰
        results = spider.results if spider and spider.results else []
        print(f"ğŸ”š å‡†å¤‡è¿”å› {len(results)} æ¡ç»“æœ")
        
        # è¿”å›ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        return {
            'books': results,
            'total_crawled': result_count,
            'total_saved': saved_count,
            'total_duplicates': duplicate_count,
            'dedup_key': 'æ ‡é¢˜ + ä½œè€…'
        }
    
    except Exception as e:
        print(f"\nâŒ çˆ¬è™«è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("="*60 + "\n")
        
        # å³ä½¿å‡ºé”™ä¹Ÿè¿”å›å·²çˆ¬å–çš„ç»“æœ
        if spider and spider.results:
            print(f"âš ï¸ è¿”å›å·²çˆ¬å–çš„ {len(spider.results)} æœ¬å›¾ä¹¦")
            return {
                'books': spider.results,
                'total_crawled': len(spider.results),
                'total_saved': spider.saved_count if spider else 0,
                'total_duplicates': spider.duplicate_count if spider else 0,
                'dedup_key': 'æ ‡é¢˜ + ä½œè€…'
            }
        return {
            'books': [],
            'total_crawled': 0,
            'total_saved': 0,
            'total_duplicates': 0,
            'dedup_key': 'æ ‡é¢˜ + ä½œè€…'
        }
    
    finally:
        print("ğŸ”š run_spider å‡½æ•°æ‰§è¡Œå®Œæ¯•ï¼Œå³å°†è¿”å›")


if __name__ == "__main__":
    # å‘½ä»¤è¡Œæ¨¡å¼
    keyword = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯: ").strip()
    if not keyword:
        keyword = "Python"  # é»˜è®¤å…³é”®è¯
    
    # è¯¢é—®æ˜¯å¦ä½¿ç”¨ MySQLï¼ˆé»˜è®¤ä½¿ç”¨ï¼‰
    use_mysql_input = input("æ˜¯å¦ä½¿ç”¨ MySQL å­˜å‚¨? (y/n, é»˜è®¤y): ").strip().lower()
    use_mysql = use_mysql_input != 'n'  # åªæœ‰è¾“å…¥ n æ‰ä¸ä½¿ç”¨
    
    # è¿è¡Œçˆ¬è™«ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
    results = run_spider(keyword, use_mysql=use_mysql)
    
    # æ‰“å°ç»“æœ
    print(f"\næ€»å…±çˆ¬å–åˆ° {len(results)} æœ¬å›¾ä¹¦")
    if use_mysql:
        print("âœ… æ•°æ®å·²ä¿å­˜åˆ° MySQL æ•°æ®åº“")
    
    for idx, book in enumerate(results, 1):
        print(f"\n{idx}. {book.get('æ ‡é¢˜', 'æœªçŸ¥')}")
        print(f"   ä½œè€…: {book.get('ä½œè€…', 'æœªçŸ¥')}")
        print(f"   ä»·æ ¼: {book.get('ç°ä»·', 'æœªçŸ¥')}")
